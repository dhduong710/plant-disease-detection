Transfer Learning, ResNet-50, and MobileNetV2

1. Transfer Learning

- Transfer learning is a machine learning technique where a model
  pre-trained on a large dataset (e.g., ImageNet) is adapted to a
  different but related task. Instead of training from scratch, we reuse
  the general features already learned.

- The early layers capture basic patterns (edges, textures, shapes),
  which are transferable. The final layers are task-specific and can be
  fine-tuned.

- A common strategy: freeze the early convolutional layers and retrain
  only the fully connected layers. This reduces training time and helps
  avoid overfitting on small datasets.


2. ResNet-50

- ResNet-50 is a 50-layer deep CNN introduced in 2015. It was designed
  to solve the vanishing gradient problem when training very deep
  networks.

- Key innovation: residual learning with skip connections. These
  shortcuts allow gradients to flow directly, making it possible to
  train extremely deep networks.

- Architecture overview:
  • Stem: 7×7 convolution + max pooling.
  • Four stages of residual blocks (3, 4, 6, 3 blocks).
  • Each residual block (bottleneck design):
    - 1×1 convolution: dimensionality reduction.
    - 3×3 convolution: feature extraction.
    - 1×1 convolution: dimensionality expansion.
  • Global average pooling + fully connected layer.

- Skip connections:
    output = f(x) + x
  • x: input feature map.
  • f(x): transformed feature map after convolutions.
  • output: final feature map.
  → In forward pass: x and f(x) are feature maps.
  → In backward pass: gradient flows through both f(x) and directly
    through x, preventing vanishing gradients.

- Bottleneck example:
  A feature map with 256 channels → reduced to 64 with 1×1 conv →
  processed with 3×3 conv → expanded back to 256 with 1×1 conv.


3. MobileNetV2

- MobileNetV2 is a lightweight CNN architecture (2018) designed for
  efficiency on mobile and embedded devices.

- Key innovations:
  • Depthwise Separable Convolutions:
    - Standard convolution is factorized into:
      1. Depthwise convolution (spatial filtering, per channel).
      2. Pointwise convolution (1×1 conv, channel mixing).
    - This drastically reduces computation while preserving accuracy.
  
  • Inverted Residual with Linear Bottlenecks:
    - Standard ResNet bottleneck: high → low → high dimensions.
    - MobileNetV2 inverts this:
      low (narrow) → high (expand) → low (linear bottleneck).
    - Skip connection is applied only if input and output dimensions match.
    - Linear bottleneck avoids losing information due to non-linear
      activation (ReLU), especially when channels are few.

- Example block:
  • Input: 32 channels.
  • Expand to 96 with 1×1 conv + ReLU6.
  • Depthwise 3×3 conv.
  • Project back to 32 channels with linear 1×1 conv.
  • If input = output shape → skip connection is applied.

- FLOPs (Floating Point Operations per Second):
  • FLOPs measure how many arithmetic operations (multiply-adds)
    a model needs for one forward pass.
  • For a standard convolution layer, FLOPs are calculated as:

      FLOPs = H_out × W_out × C_in × K × K × C_out

    where:
      H_out, W_out: output height & width
      C_in, C_out: input & output channels
      K: kernel size (e.g., 3 for 3×3)

  • For a depthwise separable convolution, the FLOPs are:

      FLOPs = (H_out × W_out × C_in × K × K)
            + (H_out × W_out × C_in × C_out)

    → Much smaller compared to standard convolution.

  • Example:
    Suppose input = 32×32×32, kernel = 3×3, output channels = 64.
    - Standard conv FLOPs:
        32 × 32 × 32 × 3 × 3 × 64 ≈ 188 M
    - Depthwise separable conv FLOPs:
        (32 × 32 × 32 × 9) + (32 × 32 × 32 × 64)
        ≈ 2.95 M + 67 M = 70 M

    → Almost 3× fewer FLOPs while maintaining similar accuracy.


4. Advantages of ResNet-50 and MobileNetV2 in Transfer Learning

- ResNet-50:
  • High representational power.
  • Robust features for diverse domains.
  • Effective for medium/large datasets.

- MobileNetV2:
  • Extremely efficient and lightweight.
  • Good choice for limited hardware or real-time applications.
  • Performs surprisingly well on small datasets.

Conclusion

Transfer learning combined with modern architectures such as ResNet-50
and MobileNetV2 enables powerful and efficient solutions in computer
vision. ResNet-50 provides depth and accuracy, while MobileNetV2 offers
efficiency and speed, making them complementary choices for different
scenarios.

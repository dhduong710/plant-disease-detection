Transfer Learning, ResNet-50, and MobileNetV2

1. Transfer Learning

- Transfer learning is a machine learning technique where a model
  pre-trained on a large dataset (e.g., ImageNet) is adapted to a
  different but related task. Instead of training from scratch, we reuse
  the general features already learned.

- The early layers capture basic patterns (edges, textures, shapes),
  which are transferable. The final layers are task-specific and can be
  fine-tuned.

- A common strategy: freeze the early convolutional layers and retrain
  only the fully connected layers. This reduces training time and helps
  avoid overfitting on small datasets.


2. ResNet-50

- ResNet-50 is a 50-layer deep CNN introduced in 2015. It was designed
  to solve the vanishing gradient problem when training very deep
  networks.

- Key innovation: residual learning with skip connections. These
  shortcuts allow gradients to flow directly, making it possible to
  train extremely deep networks.

- Architecture overview:
  • Stem: 7×7 convolution + max pooling.
  • Four stages of residual blocks (3, 4, 6, 3 blocks).
  • Each residual block (bottleneck design):
    - 1×1 convolution: dimensionality reduction.
    - 3×3 convolution: feature extraction.
    - 1×1 convolution: dimensionality expansion.
  • Global average pooling + fully connected layer.

- Skip connections:
    output = f(x) + x
  • x: input feature map.
  • f(x): transformed feature map after convolutions.
  • output: final feature map.
  → In forward pass: x and f(x) are feature maps.
  → In backward pass: gradient flows through both f(x) and directly
    through x, preventing vanishing gradients.

- Bottleneck example:
  A feature map with 256 channels → reduced to 64 with 1×1 conv →
  processed with 3×3 conv → expanded back to 256 with 1×1 conv.


3. MobileNetV2

- MobileNetV2 is a lightweight CNN architecture (2018) designed for
  efficiency on mobile and embedded devices.

- Key innovations:
  • Depthwise Separable Convolutions:
    - Standard convolution is factorized into:
      1. Depthwise convolution (spatial filtering, per channel).
      2. Pointwise convolution (1×1 conv, channel mixing).
    - This drastically reduces computation while preserving accuracy.
  
  • Inverted Residual with Linear Bottlenecks:
    - Standard ResNet bottleneck: high → low → high dimensions.
    - MobileNetV2 inverts this:
      low (narrow) → high (expand) → low (linear bottleneck).
    - Skip connection is applied only if input and output dimensions match.
    - Linear bottleneck avoids losing information due to non-linear
      activation (ReLU), especially when channels are few.

- Example block:
  • Input: 32 channels.
  • Expand to 96 with 1×1 conv + ReLU6.
  • Depthwise 3×3 conv.
  • Project back to 32 channels with linear 1×1 conv.
  • If input = output shape → skip connection is applied.

- FLOPs (Floating Point Operations per Second):
  • FLOPs measure how many arithmetic operations (multiply-adds)
    a model needs for one forward pass.
  • For a standard convolution layer, FLOPs are calculated as:

      FLOPs = H_out × W_out × C_in × K × K × C_out

    where:
      H_out, W_out: output height & width
      C_in, C_out: input & output channels
      K: kernel size (e.g., 3 for 3×3)

  • For a depthwise separable convolution, the FLOPs are:

      FLOPs = (H_out × W_out × C_in × K × K)
            + (H_out × W_out × C_in × C_out)

    → Much smaller compared to standard convolution.

  • Example:
    Suppose input = 32×32×32, kernel = 3×3, output channels = 64.
    - Standard conv FLOPs:
        32 × 32 × 32 × 3 × 3 × 64 ≈ 188 M
    - Depthwise separable conv FLOPs:
        (32 × 32 × 32 × 9) + (32 × 32 × 32 × 64)
        ≈ 2.95 M + 67 M = 70 M

    → Almost 3× fewer FLOPs while maintaining similar accuracy.


4. Focal Loss

In many real-world datasets, the class distribution is often imbalanced.  
For example, some classes may have very few samples compared to others, 
and certain categories may be visually or semantically difficult to distinguish.

Problem with standard Cross-Entropy Loss:
- Easy samples (already predicted correctly with high probability) still 
  contribute significant loss.  
- Majority classes dominate the learning process, while minority classes 
  or hard cases receive less attention.  

Solution: Focal Loss
Focal Loss modifies Cross-Entropy by adding a modulating factor:

    FL(p_t) = -α_t (1 - p_t)^γ log(p_t)

- p_t: predicted probability for the true class.  
- γ: focusing parameter (commonly set to 2). Higher values reduce the loss 
  from easy examples more aggressively.  
- α_t: class weight (used to rebalance minority classes).  

Effect:
- For easy samples (p_t ≈ 0.9), the loss is down-weighted.  
- For hard or minority-class samples (p_t ≈ 0.3), the loss remains high, 
  forcing the model to pay more attention.  


5. Fine-tuning

Fine-tuning is the process of unfreezing some of the pre-trained network’s layers 
and updating them during training.

- Phase A: Freeze most of the network and train only the classifier head 
  (the final fully connected layer).  
- Phase B: Gradually unfreeze the last few convolutional blocks 
  (e.g., layer4 in ResNet-50), and continue training with a smaller learning rate.  

Why it matters:
- Some datasets contain subtle differences between classes.  
- Fine-tuning deeper layers allows the model to adjust pre-trained filters 
  to better capture domain-specific patterns.  
- Combined with Focal Loss, this ensures the model does not overfit to 
  majority classes while still learning fine-grained features.  


6. Advantages of Combining These Techniques

- ResNet-50:
  • Strong representational power for capturing detailed features.  
  • Effective when sufficient training samples per class are available.  

- MobileNetV2:
  • Lightweight and efficient.  
  • Suitable for mobile or edge deployment in resource-constrained environments.  

- Focal Loss:
  • Addresses class imbalance by focusing training on minority and 
    hard-to-distinguish classes.  
  • Particularly useful when some categories are underrepresented or 
    difficult to separate.  

- Fine-tuning:
  • Allows the model to adapt pre-trained filters to the specific dataset.  
  • Improves accuracy on subtle or domain-specific patterns compared 
    to training only the final layer.  


Conclusion

Transfer learning combined with modern CNN architectures and specialized 
techniques makes classification tasks both accurate and efficient.

- ResNet-50 provides depth and accuracy.  
- MobileNetV2 provides efficiency and speed.  
- Focal Loss ensures the model learns effectively even with imbalanced 
  or challenging datasets.  
- Fine-tuning tailors the pre-trained knowledge to the specific characteristics 
  of the target domain.

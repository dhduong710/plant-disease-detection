Foundations: From Pixels to Channels to Tensors (for Computer Vision)

1) What is a Pixel?
- A pixel (picture element) is the smallest unit of a digital image.
- Images are grids of pixels: width × height. Example: 224 × 224 = 50,176 pixels.
- Each pixel carries intensity information (grayscale) or color components (color images).

2) Grayscale Images (Single-Channel)
- Each pixel is a single number representing brightness (intensity).
- Typical dynamic range for 8-bit images: 0 (black) … 255 (white), with mid-gray ≈ 128.
- Shape conventions:
  • Storage (NumPy/PIL): H × W
  • PyTorch tensor: 1 × H × W (C × H × W)

3) Color Images and Channels
- A channel is a 2D array (one value per pixel) stacked with others to form a color image.
- Common color models:
  • RGB: Red, Green, Blue (most common for CV/deep learning)
  • RGBA: RGB plus Alpha (opacity) channel
  • HSV / HSL: Hue, Saturation, Value/Lightness (useful for some classical CV tasks)
- For 8-bit RGB, each channel ranges 0–255. A pixel = (R, G, B).
- Example colors:
  • (255, 0, 0) = pure red
  • (0, 255, 0) = pure green
  • (0, 0, 255) = pure blue
  • (255, 255, 255) = white; (0, 0, 0) = black
- Shape conventions:
  • Storage (NumPy/PIL/Matplotlib): H × W × C  (HWC)
  • PyTorch tensors: C × H × W (CHW), where C = number of channels (1, 3, or 4)

4) Bit Depth and Dynamic Range
- Bit depth determines how many discrete values a channel can take:
  • 8-bit per channel: 0–255 (common)
  • 16-bit per channel: 0–65535 (high dynamic range, scientific/medical images)
  • Floating point images: values can be in [0,1] or other ranges
- More bit depth → finer gradations of intensity, less banding.

5) From Images to Tensors (Deep Learning)
- Neural networks operate on tensors. For images, the standard format in PyTorch is CHW.
- Conversions:
  • PIL/NumPy (HWC, uint8 [0–255]) → ToTensor() → torch.FloatTensor (CHW, [0,1])
  • Example: (R,G,B)=(128,200,64) → (128/255, 200/255, 64/255) ≈ (0.502, 0.784, 0.251)

6) Normalization: Why and How
- Purpose: stabilize and speed up training, align input distribution to what models expect.
- Channel-wise affine transform:
      x_norm = (x - mean) / std
  where x is in [0,1] after ToTensor().
- Common choices:
  • Simple symmetric scaling: mean=(0.5,0.5,0.5), std=(0.5,0.5,0.5)
    - Maps [0,1] → [-1,1]:
        0   → (0-0.5)/0.5 = -1
        0.5 → (0.5-0.5)/0.5 = 0
        1   → (1-0.5)/0.5 = +1
  • ImageNet convention (for pretrained models):
        mean = (0.485, 0.456, 0.406)
        std  = (0.229, 0.224, 0.225)
    - Use these if you fine-tune torchvision models pre-trained on ImageNet.

7) De-normalization (for Visualization)
- If you normalized with mean=(0.5,0.5,0.5), std=(0.5,0.5,0.5):
      x = x_norm * 0.5 + 0.5   # back to [0,1]
- For ImageNet normalization, invert per channel:
      x = x_norm * std + mean
- Always clamp to [0,1] before imshow to avoid artifacts.

8) Data Augmentation vs. Preprocessing
- Augmentation (train-time only): random transformations that change the image while preserving its label (e.g., random crop, flip, rotation, color jitter). Helps reduce overfitting and improve generalization.
- Preprocessing (train + eval): deterministic transforms to fit model requirements (e.g., resize to 224×224, convert to tensor, normalize). Must be consistent at inference time.
- Typical PyTorch pipeline (train):
      transforms.Compose([
          transforms.Resize((224,224)),
          transforms.RandomHorizontalFlip(p=0.5),
          transforms.RandomRotation(20),
          transforms.ColorJitter(brightness=0.2, contrast=0.2),
          transforms.ToTensor(),
          transforms.Normalize(mean, std),
      ])
  and for validation/test:
      transforms.Compose([
          transforms.Resize((224,224)),
          transforms.ToTensor(),
          transforms.Normalize(mean, std),
      ])

9) Shape and Layout Pitfalls
- Be mindful of HWC vs. CHW:
  • Matplotlib expects HWC; PyTorch tensors are CHW.
  • Use tensor.permute(1,2,0) before plt.imshow.
- Batch dimension:
  • Training uses NCHW (N=batch size). Example batch: [32, 3, 224, 224].
- Integer vs. float:
  • ToTensor() converts uint8 [0–255] → float32 [0,1]. Normalize then uses float inputs.

10) Practical Tips
- If you use a pre-trained model: adopt the exact normalization it was trained with.
- Keep augmentation in train only; don’t apply random flips/rotations in validation/test.
- When labels look “wrong” visually, remember that random augmentation can change orientation—ensure your task is invariant to those transforms.
- For medical/scientific images with different intensity ranges, compute dataset-specific mean/std.
- Save a small visualization routine to debug data pipelines (e.g., show raw, tensor, normalized, and de-normalized images).

11) Minimal PyTorch Snippets

A) Convert PIL image to normalized tensor (ImageNet stats):
    from torchvision import transforms
    normalize = transforms.Normalize((0.485,0.456,0.406),(0.229,0.224,0.225))
    pipeline = transforms.Compose([
        transforms.Resize((224,224)),
        transforms.ToTensor(),
        normalize
    ])

B) De-normalize for display (ImageNet stats):
    def denorm(img_tensor, mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)):
        # img_tensor: CHW, torch.float32
        m = img_tensor.new_tensor(mean)[:, None, None]
        s = img_tensor.new_tensor(std)[:, None, None]
        return (img_tensor * s + m).clamp(0,1)

C) Visualizing a batch:
    imgs, labels = next(iter(loader))       # imgs: [N, C, H, W]
    grid = imgs[:8]                         # take 8
    shown = denorm(grid[0]).permute(1,2,0)  # CHW -> HWC for imshow

12) Glossary
- Pixel: smallest image unit.
- Channel: a 2D layer of per-pixel values (e.g., R, G, B), stacked to form a color image.
- Dynamic range: min–max values an image/channel can take.
- Bit depth: number of bits used per channel (e.g., 8-bit → 256 levels).
- Tensor: multi-dimensional array used by deep learning frameworks.
- Normalize: scale/shift per channel to a target distribution.
- Augmentation: random but label-preserving transforms used during training.

